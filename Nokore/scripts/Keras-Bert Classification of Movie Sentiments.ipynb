{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"keras_bert_classification_tpu.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1kzGClSAy-SPo_dvgKPIgP0oJCFdZ_8Pa\n\"\"\"\n\n# @title Preparation\n!pip install -q keras-bert\n!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip\n\n# @title Constants\n\nSEQ_LEN = 128\nBATCH_SIZE = 32 # 64 seems to be the maximum possible on Kaggle\nEPOCHS = 5\nLR = 1e-4\n\n# @title Environment\nimport os\n\npretrained_path = 'uncased_L-12_H-768_A-12'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')\n\n# TF_KERAS must be added to environment variables in order to use TPU\nos.environ['TF_KERAS'] = '1'\n\n# @title Load Basic Model\nimport codecs\nfrom keras_bert import load_trained_model_from_checkpoint\n\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n\nmodel = load_trained_model_from_checkpoint(\n    config_path,\n    checkpoint_path,\n    training=True,\n    trainable=True,\n    seq_len=SEQ_LEN,\n)\n\n# @title Download IMDB Data\nimport tensorflow as tf\n\ndataset = tf.keras.utils.get_file(\n    fname=\"aclImdb.tar.gz\", \n    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n    extract=True,\n)","execution_count":1,"outputs":[{"output_type":"stream","text":"Archive:  uncased_L-12_H-768_A-12.zip\n   creating: uncased_L-12_H-768_A-12/\n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"},{"output_type":"stream","text":"Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n84131840/84125825 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# @title Convert Data to Array\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras_bert import Tokenizer\n\ntokenizer = Tokenizer(token_dict)\n\n\ndef load_data(path):\n    global tokenizer\n    indices, sentiments = [], []\n    for folder, sentiment in (('neg', 0), ('pos', 1)):\n        folder = os.path.join(path, folder)\n        for name in tqdm(os.listdir(folder)):\n            with open(os.path.join(folder, name), 'r') as reader:\n                  text = reader.read()\n            ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n            indices.append(ids)\n            sentiments.append(sentiment)\n    items = list(zip(indices, sentiments))\n    np.random.shuffle(items)\n    indices, sentiments = zip(*items)\n    indices = np.array(indices)\n    return [indices, np.zeros_like(indices)], np.array(sentiments)\n  \n  \n  \ntrain_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'train')\ntest_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'test')\n\n\ntrain_x, train_y = load_data(train_path)\ntest_x, test_y = load_data(test_path)\n\n","execution_count":2,"outputs":[{"output_type":"stream","text":"100%|██████████| 12500/12500 [00:39<00:00, 317.53it/s]\n100%|██████████| 12500/12500 [00:40<00:00, 311.27it/s]\n100%|██████████| 12500/12500 [00:38<00:00, 325.21it/s]\n100%|██████████| 12500/12500 [00:38<00:00, 322.08it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_x list details:\")\nprint(len(train_x))\nprint(train_x[:5])\nprint(train_y[:5])\nprint(train_y.shape)\n\n# @title Build Custom Model\nfrom tensorflow.python import keras\nfrom keras_bert import AdamWarmup, calc_train_steps\n\ninputs = model.inputs[:2]\ndense = model.get_layer('NSP-Dense').output\noutputs = keras.layers.Dense(units=2, activation='softmax')(dense)\n\ndecay_steps, warmup_steps = calc_train_steps(\n    train_y.shape[0],\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n)\n\nmodel = keras.models.Model(inputs, outputs)\n\n#freeze some layers\nfor layer in model.layers:\n    layer.trainable = False\n\nmodel.layers[-1].trainable = True\nmodel.layers[-2].trainable = True\nmodel.layers[-3].trainable = True\n\nmodel.compile(\n    AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n    loss='sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n)\n\n# @title Initialize Variables\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nsess = K.get_session()\nuninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\ninit_op = tf.variables_initializer(\n    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n)\nsess.run(init_op)","execution_count":3,"outputs":[{"output_type":"stream","text":"train_x list details:\n2\n[array([[  101,  1000, 23564, ...,  2006,  2019,   102],\n       [  101,  2045,  2024, ...,  2008,  2009,   102],\n       [  101,  2019,  2256, ...,  5006,  7664,   102],\n       ...,\n       [  101,  2023,  2003, ...,     0,     0,     0],\n       [  101,  3475,  1005, ...,  2370,  1999,   102],\n       [  101,  2023,  2143, ...,  2612,  1010,   102]]), array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])]\n[1 0 1 1 1]\n(25000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert import get_custom_objects\n# @title Fit\n\nwith tf.keras.utils.custom_object_scope(get_custom_objects()):\n    model.fit(\n        train_x,\n        train_y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n    )\n\n# @title Predict\n\nwith tf.keras.utils.custom_object_scope(get_custom_objects()):\n    predicts = model.predict(test_x, verbose=True).argmax(axis=-1)\n\n# @title Accuracy\n\nprint(np.sum(test_y == predicts) / test_y.shape[0])","execution_count":4,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n25000/25000 [==============================] - 120s 5ms/sample - loss: 0.5332 - sparse_categorical_accuracy: 0.7324\nEpoch 2/5\n25000/25000 [==============================] - 118s 5ms/sample - loss: 0.4597 - sparse_categorical_accuracy: 0.7839\nEpoch 3/5\n25000/25000 [==============================] - 118s 5ms/sample - loss: 0.4495 - sparse_categorical_accuracy: 0.7923\nEpoch 4/5\n25000/25000 [==============================] - 118s 5ms/sample - loss: 0.4433 - sparse_categorical_accuracy: 0.7946\nEpoch 5/5\n25000/25000 [==============================] - 118s 5ms/sample - loss: 0.4396 - sparse_categorical_accuracy: 0.7948\n25000/25000 [==============================] - 113s 5ms/sample\n0.80892\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":5,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb  uncased_L-12_H-768_A-12  uncased_L-12_H-768_A-12.zip\r\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}