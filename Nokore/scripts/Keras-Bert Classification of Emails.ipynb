{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Load Email Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport random\n\n### Read-in the emails and print some basic statistics\n\nNsamp = 5000\nmax_cells = 50\nmaxlen = 20\n\n# Install Simon\n#!pip install git+https://github.com/algorine/simon\n#from Simon.LengthStandardizer import DataLengthStandardizerRaw\n\n# Enron\nEnronEmails = pd.read_csv('../input/enron-email-bodies/enron_emails_body.csv',dtype='str', header=0)\nprint(\"The size of the Enron emails dataframe is:\")\nprint(EnronEmails.shape)\nprint(\"Ten Enron emails are:\")\nprint(EnronEmails.loc[:10])\n\n# Spam\nSpamEmails = pd.read_csv('../input/fraudulent-email-bodies/fraudulent_emails_body.csv',encoding=\"ISO-8859-1\",dtype='str', header=0)\nprint(\"The size of the Spam emails dataframe is:\")\nprint(SpamEmails.shape)\nprint(\"Ten Spam emails are:\")\nprint(SpamEmails.loc[:10])\n\n# Convert everything to lower-case, put one sentence per column in a tabular\n# structure, truncate to max_cells...\nProcessedEnronEmails=[row.lower().split('\\n')[:max_cells] for row in EnronEmails.iloc[:,1]]\n#print(\"3 Enron emails after Processing (in list form) are:\")\n#print((ProcessedEnronEmails[:3]))\nEnronEmails = pd.DataFrame(random.sample(ProcessedEnronEmails,Nsamp)).transpose()\n\n\n#EnronEmails = DataLengthStandardizerRaw(EnronEmails,max_cells)\n\n\n#print(\"Ten Enron emails after Processing (in DataFrame form) are:\")\n#print((EnronEmails[:10]))\nprint(\"Enron email dataframe after Processing shape:\")\nprint(EnronEmails.shape)\n\nProcessedSpamEmails=[row.lower().split('/n')[:max_cells] for row in SpamEmails.iloc[:,1]]\n#print(\"3 Spam emails after Processing (in list form) are:\")\n#print((ProcessedSpamEmails[:3]))\nSpamEmails = pd.DataFrame(random.sample(ProcessedSpamEmails,Nsamp)).transpose()\n\n\n#SpamEmails = DataLengthStandardizerRaw(SpamEmails,max_cells)\n\n\n#print(\"Ten Spam emails after Processing (in DataFrame form) are:\")\n#print((SpamEmails[:10]))\nprint(\"Spam email dataframe after Processing shape:\")\nprint(SpamEmails.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"The size of the Enron emails dataframe is:\n(20000, 2)\nTen Enron emails are:\n   Unnamed: 0                                                  0\n0           0                          here is our forecast\\n\\n \n1           1  traveling to have a business meeting takes the...\n2           2                     test successful.  way to go!!!\n3           3  randy,\\n\\n can you send me a schedule of the s...\n4           4                let's shoot for tuesday at 11:45.  \n5           5  greg,\\n\\n how about either next tuesday or thu...\n6           6  please cc the following distribution list with...\n7           7                   any morning between 10 and 11:30\n8           8  1. login:  pallen pw: ke9davis\\n\\n i don't thi...\n9           9  ---------------------- forwarded by phillip k ...\n10         10  mr. buckner,\\n\\n for delivered gas behind san ...\nThe size of the Spam emails dataframe is:\n(5187, 2)\nTen Spam emails are:\n   Unnamed: 0                                               text\n0           1  /nURGENT BUSINESS ASSISTANCE AND PARTNERSHIP./...\n1           2  /nDear Friend,/n/nI am Mr. Ben Suleman a custo...\n2           3  /nFROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ...\n3           4  /nFROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ...\n4           5  /nDear sir, /n /nIt is with a heart full of ho...\n5           6  PRESIDENT/MANAGING DIRECTOR /n   /nDear Sir/Ma...\n6           7  previous military regimes in our country, gove...\n7           8  1.  70% for us (the officials) /n2.  20% for t...\n8           9  /nDear Sir,/n/nI am Barrister Tunde Dosumu (SA...\n9          10  /nI ascetained your contact through a reliable...\n10         11  /nCHALLENGE SECURITIES LTD./nLAGOS, NIGERIA/n/...\nEnron email dataframe after Processing shape:\n(50, 5000)\nSpam email dataframe after Processing shape:\n(50, 5000)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Preparation of Keras-Bert**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n!pip install -q keras-bert\n!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip\n\n# Constants\n\nSEQ_LEN = 128\nBATCH_SIZE = 32 # 64 seems to be the maximum possible on Kaggle\nEPOCHS = 10\nLR = 1e-4\n\n# Environment\nimport os\n\npretrained_path = 'uncased_L-12_H-768_A-12'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')\n\n# TF_KERAS must be added to environment variables in order to use TPU\nos.environ['TF_KERAS'] = '1'\n\n# Load Basic Model\nimport codecs\nfrom keras_bert import load_trained_model_from_checkpoint\n\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n\nmodel = load_trained_model_from_checkpoint(\n    config_path,\n    checkpoint_path,\n    training=True,\n    trainable=True,\n    seq_len=SEQ_LEN,\n)\n\nimport tensorflow as tf","execution_count":8,"outputs":[{"output_type":"stream","text":"Archive:  uncased_L-12_H-768_A-12.zip\n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Convert Data to appropriate Array(s) for Keras-Bert input**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nraw_data = np.column_stack((SpamEmails,EnronEmails)).T\nprint(\"DEBUG::raw_data:\")\nprint(raw_data.shape)\n\n# corresponding labels\nCategories = ['spam','notspam']\nheader = ([0]*Nsamp)\nheader.extend(([1]*Nsamp))","execution_count":9,"outputs":[{"output_type":"stream","text":"DEBUG::raw_data:\n(10000, 50)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nfrom keras_bert import Tokenizer\n\ntokenizer = Tokenizer(token_dict)\n\n# function for processing data into the right format\ndef load_data(raw_data,header):\n    global tokenizer\n    indices, labels = [], []\n    for i in range(raw_data.shape[0]):\n        out=''\n        for text in raw_data[i,:]:\n            out = str(text)[:maxlen]+out\n        ids, segments = tokenizer.encode(out, max_len=SEQ_LEN)\n        indices.append(ids)\n        labels.append(header[i])\n        #print(i)\n    items = list(zip(indices, labels))\n    np.random.shuffle(items)\n    indices, labels = zip(*items)\n    indices = np.array(indices)\n    return [indices, np.zeros_like(indices)], np.array(labels)\n\n\n# shuffle raw data first\ndef unison_shuffled_copies(a, b):\n    p = np.random.permutation(len(b))\n    data = a[p,:]\n    header = np.asarray(b)[p]\n    return data, list(header)\n\nraw_data, header = unison_shuffled_copies(raw_data, header)\n\n\nidx = int(0.9*raw_data.shape[0])\ntrain_x, train_y = load_data(raw_data[:idx,:],header[:idx]) # 90% of data for training\ntest_x, test_y = load_data(raw_data[idx:,:],header[idx:]) # remaining 10% for testing\n\nprint(\"train_x/train_y list details, to make sure it is of the right form:\")\nprint(len(train_x))\nprint(train_x)\nprint(train_y[:5])\nprint(train_y.shape)","execution_count":10,"outputs":[{"output_type":"stream","text":"train_x/train_y list details, to make sure it is of the right form:\n2\n[array([[  101,  3904,  8540, ...,     0,     0,     0],\n       [  101,  2004, 18777, ...,  4140,  2022,   102],\n       [  101,  3904, 11039, ...,  2031,  2042,   102],\n       ...,\n       [  101,  3904,  8540, ...,     0,     0,     0],\n       [  101,  2097,  3942, ..., 22254, 17872,   102],\n       [  101,  3904,  8540, ...,     0,     0,     0]]), array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])]\n[0 0 0 0 1]\n(9000,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Freeze some layers (potentially), Train and Predict**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Custom Model\nfrom tensorflow.python import keras\nfrom keras_bert import AdamWarmup, calc_train_steps\n\ninputs = model.inputs[:2]\ndense = model.get_layer('NSP-Dense').output\noutputs = keras.layers.Dense(units=2, activation='softmax')(dense)\n\ndecay_steps, warmup_steps = calc_train_steps(\n    train_y.shape[0],\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n)\n\nmodel = keras.models.Model(inputs, outputs)\n\n#freeze some layers\nFREEZE=True\nif FREEZE:\n    for layer in model.layers:\n        layer.trainable = False\n\n    model.layers[-1].trainable = True\n    model.layers[-2].trainable = True\n    model.layers[-3].trainable = True\n\nmodel.compile(\n    AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n    loss='sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n)\n\n# @title Initialize Variables\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nsess = K.get_session()\nuninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\ninit_op = tf.variables_initializer(\n    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n)\nsess.run(init_op)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert import get_custom_objects\n\n# Fit\nwith tf.keras.utils.custom_object_scope(get_custom_objects()):\n    model.fit(\n        train_x,\n        train_y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n    )\n\n# Predict\nwith tf.keras.utils.custom_object_scope(get_custom_objects()):\n    predicts = model.predict(test_x, verbose=True).argmax(axis=-1)\n\n# Accuracy\nprint(np.sum(test_y == predicts) / test_y.shape[0])","execution_count":12,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n9000/9000 [==============================] - 45s 5ms/sample - loss: 0.4350 - sparse_categorical_accuracy: 0.8024\nEpoch 2/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1893 - sparse_categorical_accuracy: 0.9349\nEpoch 3/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1516 - sparse_categorical_accuracy: 0.9462\nEpoch 4/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1397 - sparse_categorical_accuracy: 0.9498\nEpoch 5/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1283 - sparse_categorical_accuracy: 0.9564\nEpoch 6/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1271 - sparse_categorical_accuracy: 0.9537\nEpoch 7/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1226 - sparse_categorical_accuracy: 0.9547\nEpoch 8/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1250 - sparse_categorical_accuracy: 0.9551\nEpoch 9/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1216 - sparse_categorical_accuracy: 0.9564\nEpoch 10/10\n9000/9000 [==============================] - 43s 5ms/sample - loss: 0.1170 - sparse_categorical_accuracy: 0.9594\n1000/1000 [==============================] - 6s 6ms/sample\n0.965\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":13,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb  uncased_L-12_H-768_A-12.zip\r\nuncased_L-12_H-768_A-12    uncased_L-12_H-768_A-12.zip.1\r\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}